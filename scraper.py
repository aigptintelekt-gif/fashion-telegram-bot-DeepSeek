import requests
from bs4 import BeautifulSoup

HEADERS = {
    "User-Agent": "Mozilla/5.0"
}

def fetch_site(url, selectors, site_name, max_items=10):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å –∞–∫—Ç–∏–≤–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ (Markdown).
    """
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        items = []

        for x in soup.select(selectors)[:max_items]:
            text = x.get_text(strip=True)
            link = x.get("href")
            if not text:
                continue
            if link:
                if not link.startswith("http"):
                    link = url.rstrip("/") + link
                items.append(f"[{text}]({link})")
            else:
                items.append(text)

        if not items:
            print(f"‚ö†Ô∏è {site_name}: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º {selectors}")
            return None

        print(f"‚úÖ {site_name}: –Ω–∞–π–¥–µ–Ω–æ {len(items)} –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤")
        return f"‚ú® **{site_name}**\n" + "\n".join(items)

    except Exception as e:
        print(f"‚ùå {site_name}: –æ—à–∏–±–∫–∞ {e}")
        return None
def fetch_article_text(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
        paragraphs = soup.find_all("p")
        text = "\n".join([p.get_text(strip=True) for p in paragraphs])
        return text[:3000]  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è LLM, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
        return ""

# ----------------- –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –º–æ–¥–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π -----------------
def fetch_wgsn():
    return fetch_site("https://www.wgsn.com/en", "h2, h3 a", "WGSN")

def fetch_coloro():
    return fetch_site("https://coloro.com/", "h2, h3 a", "Coloro")

def fetch_bof():
    return fetch_site("https://www.businessoffashion.com/", "h3 a", "Business of Fashion")

def fetch_nike():
    return fetch_site("https://about.nike.com/en/newsroom", "h2 a", "Nike News")

def fetch_footy():
    return fetch_site("https://www.footyheadlines.com/", "h3 a", "FootyHeadlines")

def fetch_sports_style():
    return fetch_site("https://www.sports.ru/style/", "h2 a, h3 a", "Sports.ru ‚Äî –°—Ç–∏–ª—å")

def fetch_wwd():
    return fetch_site("https://wwd.com/", "h3 a", "WWD")

def fetch_blueprint():
    return fetch_site("https://theblueprint.ru/", "h2 a, h3 a", "Blueprint")

# ----------------- –°–±–æ—Ä –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π -----------------
def get_all_fashion_updates():
    updates = [
        fetch_wgsn(),
        fetch_coloro(),
        fetch_bof(),
        fetch_nike(),
        fetch_footy(),
        fetch_sports_style(),
        fetch_wwd(),
        fetch_blueprint(),
    ]
    # –£–±–∏—Ä–∞–µ–º None
    filtered = [u for u in updates if u]
    return "\n\n".join(filtered) if filtered else "–ù–µ—Ç —Å–≤–µ–∂–∏—Ö –º–æ–¥–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π üòî"

# ----------------- –¢–µ—Å—Ç –ø–∞—Ä—Å–µ—Ä–∞ -----------------
if __name__ == "__main__":
    print("üöÄ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞ –º–æ–¥–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π:")
    news = get_all_fashion_updates()
    print(news)
